{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baef6e1b-a76f-4e02-9da4-d79d5fa090b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "# Ruta de la carpeta principal que contiene los datos\n",
    "folder_path = \"../Data/PetroImages_Augmentedx3\"\n",
    "# Ruta base para las 5 carpetas\n",
    "output_base_path = \"../Data/PetroImages_KFold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ea63fe2-503f-46a5-b00c-cd40e3045f66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Proporción de datos para cada carpeta (porcentaje)\n",
    "split_percentage = 20\n",
    "random.seed(123)\n",
    "\n",
    "# Lista de carpetas por clase\n",
    "class_folders = os.listdir(folder_path)\n",
    "\n",
    "# Itera a través de cada carpeta por clase\n",
    "for class_folder in class_folders:\n",
    "    class_path = os.path.join(folder_path, class_folder)\n",
    "\n",
    "    # Lista de archivos en la carpeta actual\n",
    "    files = os.listdir(class_path)\n",
    "    # Baraja (shuffle) los archivos para dividir aleatoriamente\n",
    "    random.shuffle(files)\n",
    "\n",
    "    # Calcula la cantidad de datos para cada carpeta\n",
    "    num_files = len(files)\n",
    "    \n",
    "    num_files_per_split = math.ceil(num_files * split_percentage / 100)\n",
    "    \n",
    "    # Crea las rutas para las 5 carpetas\n",
    "    output_folders = [os.path.join(output_base_path, f\"K_{i}\", class_folder) for i in range(5)]\n",
    "\n",
    "    # Crea las carpetas si no existen\n",
    "    for folder in output_folders:\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    # Mueve los archivos a las carpetas correspondientes\n",
    "    for i in range(5):\n",
    "        split_files = files[i * num_files_per_split: (i + 1) * num_files_per_split]\n",
    "        output_folder = output_folders[i]\n",
    "\n",
    "        for file in split_files:\n",
    "            src = os.path.join(class_path, file)\n",
    "            dst = os.path.join(output_folder, file)\n",
    "            shutil.copy(src, dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c0241b2-ab26-4bd6-a01b-8b264147768e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unir_carpetas(carpetas_origen, carpeta_destino):\n",
    "    # Crear la carpeta de destino si no existe\n",
    "    if not os.path.exists(carpeta_destino):\n",
    "        os.makedirs(carpeta_destino)\n",
    "\n",
    "    # Iterar sobre las carpetas de origen\n",
    "    for carpeta in carpetas_origen:\n",
    "        # Iterar sobre las subcarpetas dentro de cada carpeta de origen\n",
    "        for subcarpeta in os.listdir(carpeta):\n",
    "            subcarpeta_ruta = os.path.join(carpeta, subcarpeta)\n",
    "\n",
    "            # Ignorar archivos no directorios\n",
    "            if not os.path.isdir(subcarpeta_ruta):\n",
    "                continue\n",
    "\n",
    "            # Crear la subcarpeta en la carpeta de destino si no existe\n",
    "            subcarpeta_destino = os.path.join(carpeta_destino, subcarpeta)\n",
    "            if not os.path.exists(subcarpeta_destino):\n",
    "                os.makedirs(subcarpeta_destino)\n",
    "\n",
    "            # Copiar o mover las imágenes de la subcarpeta de origen a la subcarpeta de destino\n",
    "            for archivo in os.listdir(subcarpeta_ruta):\n",
    "                origen = os.path.join(subcarpeta_ruta, archivo)\n",
    "                destino = os.path.join(subcarpeta_destino, archivo)\n",
    "\n",
    "                # Puedes elegir entre copiar (shutil.copy) o mover (shutil.move) según tus necesidades\n",
    "                shutil.copy(origen, destino)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b632f46b-f5eb-413e-b606-328b1a9dad78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Especifica las carpetas de origen y destino\n",
    "carpetas_origen = [\"../Data/PetroImages_KFold/K_0\", \"../Data/PetroImages_KFold/K_1\", \"../Data/PetroImages_KFold/K_2\", \"../Data/PetroImages_KFold/K_3\"]\n",
    "Kfold0123 = \"../Data/PetroImages_KFold/Kfold0123\"\n",
    "\n",
    "# Llama a la función para unir las carpetas\n",
    "unir_carpetas(carpetas_origen, Kfold0123)\n",
    "k_4 = \"../Data/PetroImages_KFold/K_4\"\n",
    "\n",
    "# Especifica las carpetas de origen y destino\n",
    "carpetas_origen = [\"../Data/PetroImages_KFold/K_0\", \"../Data/PetroImages_KFold/K_1\", \"../Data/PetroImages_KFold/K_2\", \"../Data/PetroImages_KFold/K_4\"]\n",
    "Kfold0124 = \"../Data/PetroImages_KFold/Kfold0124\"\n",
    "\n",
    "# Llama a la función para unir las carpetas\n",
    "unir_carpetas(carpetas_origen, Kfold0124)\n",
    "k_3 = \"../Data/PetroImages_KFold/K_3\"\n",
    "\n",
    "# Especifica las carpetas de origen y destino\n",
    "carpetas_origen = [\"../Data/PetroImages_KFold/K_0\", \"../Data/PetroImages_KFold/K_1\", \"../Data/PetroImages_KFold/K_3\", \"../Data/PetroImages_KFold/K_4\"]\n",
    "Kfold0134 = \"../Data/PetroImages_KFold/Kfold0134\"\n",
    "\n",
    "# Llama a la función para unir las carpetas\n",
    "unir_carpetas(carpetas_origen, Kfold0134)\n",
    "k_2 = \"../Data/PetroImages_KFold/K_2\"\n",
    "\n",
    "# Especifica las carpetas de origen y destino\n",
    "carpetas_origen = [\"../Data/PetroImages_KFold/K_0\", \"../Data/PetroImages_KFold/K_2\", \"../Data/PetroImages_KFold/K_3\", \"../Data/PetroImages_KFold/K_4\"]\n",
    "Kfold0234 = \"../Data/PetroImages_KFold/Kfold0234\"\n",
    "\n",
    "# Llama a la función para unir las carpetas\n",
    "unir_carpetas(carpetas_origen, Kfold0234)\n",
    "k_2 = \"../Data/PetroImages_KFold/K_1\"\n",
    "\n",
    "# Especifica las carpetas de origen y destino\n",
    "carpetas_origen = [\"../Data/PetroImages_KFold/K_1\", \"../Data/PetroImages_KFold/K_2\", \"../Data/PetroImages_KFold/K_3\", \"../Data/PetroImages_KFold/K_4\"]\n",
    "Kfold1234 = \"../Data/PetroImages_KFold/Kfold1234\"\n",
    "\n",
    "# Llama a la función para unir las carpetas\n",
    "unir_carpetas(carpetas_origen, Kfold1234)\n",
    "k_2 = \"../Data/PetroImages_KFold/K_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8758789f-7483-4f9f-9940-a0fc4a0cf401",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19204 files belonging to 106 classes.\n"
     ]
    }
   ],
   "source": [
    "#Cargar dataset\n",
    "#Dividir dataset para entrenamiento y validación  \n",
    "BATCH_SIZE = 64\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "# Crea el conjunto de datos de TensorFlow desde los directorios\n",
    "k_ds = tf.keras.utils.image_dataset_from_directory(carpeta_destino,\n",
    "                                                            shuffle=True,\n",
    "                                                            seed = 123,\n",
    "                                                            batch_size=BATCH_SIZE,\n",
    "                                                            image_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "55da697f-c456-4304-93b5-c536192273f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b22720c1-eece-43ef-8cbc-416ec2a52946",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4801 files belonging to 106 classes.\n",
      "Found 4801 files belonging to 106 classes.\n",
      "Found 4801 files belonging to 106 classes.\n",
      "Found 4801 files belonging to 106 classes.\n",
      "Found 4690 files belonging to 106 classes.\n"
     ]
    }
   ],
   "source": [
    "#Cargar dataset\n",
    "#Dividir dataset para entrenamiento y validación  \n",
    "BATCH_SIZE = 64\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "# Crea el conjunto de datos de TensorFlow desde los directorios\n",
    "k0_ds = tf.keras.utils.image_dataset_from_directory(k_0,\n",
    "                                                            shuffle=True,\n",
    "                                                            seed = 123,\n",
    "                                                            batch_size=BATCH_SIZE,\n",
    "                                                            image_size=IMG_SIZE)\n",
    "# Crea el conjunto de datos de TensorFlow desde los directorios\n",
    "k1_ds = tf.keras.utils.image_dataset_from_directory(k_1,\n",
    "                                                            shuffle=True,\n",
    "                                                            seed = 123,\n",
    "                                                            batch_size=BATCH_SIZE,\n",
    "                                                            image_size=IMG_SIZE)\n",
    "# Crea el conjunto de datos de TensorFlow desde los directorios\n",
    "k2_ds = tf.keras.utils.image_dataset_from_directory(k_2,\n",
    "                                                            shuffle=True,\n",
    "                                                            seed = 123,\n",
    "                                                            batch_size=BATCH_SIZE,\n",
    "                                                            image_size=IMG_SIZE)\n",
    "# Crea el conjunto de datos de TensorFlow desde los directorios\n",
    "k3_ds = tf.keras.utils.image_dataset_from_directory(k_3,\n",
    "                                                            shuffle=True,\n",
    "                                                            seed = 123,\n",
    "                                                            batch_size=BATCH_SIZE,\n",
    "                                                            image_size=IMG_SIZE)\n",
    "# Crea el conjunto de datos de TensorFlow desde los directorios\n",
    "k4_ds = tf.keras.utils.image_dataset_from_directory(k_4,\n",
    "                                                            shuffle=True,\n",
    "                                                            seed = 123,\n",
    "                                                            batch_size=BATCH_SIZE,\n",
    "                                                            image_size=IMG_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b12eb0-a398-4969-964a-5fe676b2b0dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
